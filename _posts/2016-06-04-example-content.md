---
layout: post
title: "Joint Attention Discovery"  
categories: others
---
# Joint Attention Discovery
<img class="img-responsive" src="/asset/EPIC2017_concept.jpg">
### Abstract
This work aims to develop a computer-vision technique for understanding objects jointly attended by a group of people during social interactions. As a key tool to discover such objects of joint attention, we rely on a collection of wearable eye-tracking cameras that provide a first-person video of interaction scenes and points-of-gaze data of interacting parties. Technically, we propose a hierarchical conditional random field-based model that can 1) localize events of joint attention temporally and 2) segment objects of joint attention spatially. We show that by alternating these two procedures, objects of joint attention can be discovered reliably even from cluttered scenes and noisy points-of-gaze data. Experimental results demonstrate that our approach outperforms several state-of-the-art methods for co-segmentation and joint attention discovery.

## Publication:
Y. Huang, <u>M. Cai</u>, H. Kera, R. Yonetani, K. Higuchi, and Y. Sato, &quot;Temporal localization and spatial segmentation of joint attention in multiple first-person videos,&quot; <i>Proceedings of IEEE International Conference on Computer Vision Workshop (**ICCVW**)</i>, pp. 2313-2321, 2017.   
[[pdf]](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w34/attention_hyfiis.u-tokyo.ac.jp_cai-mjiis.u-tokyo.ac.jp_keraiis.u-tokyo.ac.jp_ICCV_2017_paper.pdf)
[[poster]](https://github.com/cai-mj/cai-mj.github.io/blob/master/files/HC_ICCVW2017_poster.pdf)
[[dataset]](https://github.com/cai-mj/UTJA_dataset)
